{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used three datasets in this paper (one for each growth cycle -2017/18/19). \n",
    "\n",
    "The code described below was applied to each dataset separetely, but here we show just one to avoid repetition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Import libraries for tabular data processing\n",
    "import pandas as pd\n",
    "import tabula as tab\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Convert pdf to csv\n",
    "\n",
    "The datasets made available by Genomes to Fields initiative (G2F) contained some tables in pdf instead of csv format. So to use the tables we are converting it from pdf to csv using tabula library, the conversion was performed page by page so it would be easier to check the values accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# For a document with 18 pages\n",
    "pages = [] \n",
    "for i in range(18):\n",
    "    each_page = tabula.read_pdf('/data/fieldnotes.pdf', pages=i+1, output_format='dataframe')\n",
    "    print(f\"Doing page {i+1}\") # print the current page to notice if the function got stuck\n",
    "    pages.append(each_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Clean and tidy each page\n",
    "for i in range(18):\n",
    "    pages[i] = pages[i][0]\n",
    "\n",
    "# Export this to csv and check if the values between pdf and csv version are matching\n",
    "for i in range(18):\n",
    "    pages[i].to_csv(f\"/data/fieldnotes_page{i}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Check the if the csv file matches the pdf version manually, them merge the pages together in a file called \"/data/fieldnotes_mergedpages.csv\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Clean csv\n",
    "The tabular dataset contains missing values, and other inconsistencies that must be resolved before using it as input for a machine learning model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```Check for NAs```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def na_stats(df):\n",
    "    '''\n",
    "    Function to count the NAs per column\n",
    "    and count the amount of unique values\n",
    "    '''\n",
    "    list_of_columns = df.columns\n",
    "    print(f\"This dataframe has {len(df)} rows\")\n",
    "    \n",
    "    # Check each column for na values\n",
    "    for column in list_of_columns:\n",
    "        df_clean = df[df[column].notna()]\n",
    "        nans = len(df[df[column].isna()])\n",
    "        unique_values = df_clean[column].unique()\n",
    "        \n",
    "        print(f\"{column} has {len(unique_values)} unique values, and {nans} NAs \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Read the csv file, each dataframe is separated by Year already\n",
    "df = pd.read_csv(\"/data/fieldnotes_mergedpages.csv\")\n",
    "na_stats(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Fill Yield NAs in the dataframe using its replicate values\n",
    "df['Yield'] = df['Yield'].fillna(\n",
    "                df.groupby(by=['Test', 'Pedigree'])['Yield'].transform(\n",
    "                lambda s:s.loc[s.first_valid_index()]))\n",
    "                #takes the first valid number in the group and fill in the NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Drop samples that didn't have a replicate Yield value\n",
    "df = df.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Replace NaN in Stock column with 'unknow'\n",
    "df['Stock'] = df['Stock'].fillna('unknown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```Clean Typos```\n",
    "\n",
    "Converting pdf to csv may have created some typos like extra blank spaces in the end of the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Clean typos in the string columns\n",
    "columns = ['Barcode', 'Test', 'Stock', 'Pedigree']\n",
    "\n",
    "for col in columns:\n",
    "    df[col] = df[col].str.strip('!? \\n\\t\"')\n",
    "    df[col] = df[col].str.strip(\"'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```Separate Hybrids into parental lines```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Repeat this for each df used\n",
    "df['Parental 1'] = df['Pedigree'].str.split('/').str.get(0)\n",
    "df['Parental 2'] = df['Pedigree'].str.split('/').str.get(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```Add columns to describe treatment```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# G2LA, G2FE and DG2F are the original treatment names as used in the Genomes to Field Initiative.\n",
    "# In the paper they are replaced by more intuitive names P2F1, P1F1 and P1F2 \n",
    "g2la = df.loc[df['Test'] == 'G2LA']\n",
    "g2la['Planting'] ='late'\n",
    "g2la['Fertilizer'] = 'optimal'\n",
    "\n",
    "g2fe = df.loc[df['Test'] == 'G2FE']\n",
    "g2fe['Planting'] = 'optimal'\n",
    "g2fe['Fertilizer'] = 'optimal'\n",
    "\n",
    "dg2f = df.loc[df['Test'] == 'DG2F']\n",
    "dg2f['Planting'] = 'optimal'\n",
    "dg2f['Fertilizer'] ='reduced'\n",
    "\n",
    "df = g2la.append(g2fe).append(dg2f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# save csv\n",
    "df.to_csv('/data/fielddata/df_2017.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for preparing input data\n",
    "from fastai.tabular.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned dataframes\n",
    "df_2017 = pd.read_csv('/data/fielddata/df_2017.csv')\n",
    "df_2018 = pd.read_csv('/data/fielddata/df_2018.csv')\n",
    "df_2019 = pd.read_csv('/data/fielddata/df_2019.csv')\n",
    "\n",
    "# Blend the datasets together\n",
    "mixed = df_2017.append(df_2018)\n",
    "mixed_df = mixed.append(df_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Divide between train/validation and test (90:10%)\n",
    "# the sample function uses a equal probability of getting any row in the dataset\n",
    "df_test = mixed_df.sample(frac=0.1, random_state=32)\n",
    "df_train_val = mixed_df.drop(df_test.index)\n",
    "\n",
    "df_test.to_csv('/data/fielddata/df_test.csv', index=False)\n",
    "df_train_val.to_csv('/data/fielddata/df_train_val.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Random splitter function from fastai\n",
    "splitter = RandomSplitter(seed=42)\n",
    "splits = splitter(range_of(df_train_validation))\n",
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "procs = [Categorify, Normalize, FillMissing]\n",
    "cat_names = [ 'Parental 1', 'Parental 2', 'Planting', 'Stock', 'Fertilizer']\n",
    "cont_names =['Days_after_sowing']\n",
    "\n",
    "dls = TabularDataLoaders.from_df(df_train_val,\n",
    "                                        y_names=\"Yield\",\n",
    "                                        cat_names=cat_names,\n",
    "                                        cont_names=cont_names,\n",
    "                                        procs = procs,\n",
    "                                        splits = splits)\n",
    "\n",
    "# Prepare the train/val data for XGBoost and Random Forest\n",
    "X_train, y_train = dls.train.xs, dls.train.ys.values.ravel()\n",
    "X_val, y_val = dls.valid.xs, dls.valid.ys.values.ravel()\n",
    "\n",
    "X = X_train.append(X_val)\n",
    "Y = np.append(y_train, y_val)\n",
    "\n",
    "X.to_csv('/data/fielddata/X_ordinal.csv')\n",
    "Y.to_csv('/data/fielddata/Y.csv')\n",
    "\n",
    "# Prepare the holdout data for XGBoost and Random Forest\n",
    "dl = dls.test_dl(df_test)\n",
    "Xtest = dl.xs\n",
    "\n",
    "Ytest = Xtest.pop('Yield')\n",
    "Xtest.to_csv('/data/fielddata/Xtest_ordinal.csv')\n",
    "Ytest.to_csv('/data/fielddata/Ytest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare One-hot encoded dataset\n",
    "# One hot encode\n",
    "categorical_cols = ['Stock', 'Parental 1','Parental 2', 'Planting', 'Fertilizer', 'Days_after_sowing']\n",
    "\n",
    "# Ad the test set to the training dataset to dummy then together, so they match\n",
    "superX = X.append(Xtest)\n",
    "superX = pd.get_dummies(superX, columns=categorical_cols)\n",
    "\n",
    "X_ohe = superX[:3878]\n",
    "Xtest_ohe = superX[3878:]\n",
    "\n",
    "# Save the one-hot encoded Xs\n",
    "# No need to export the target values as they are the same as above\n",
    "X_ohe.to_csv('/data/fielddata/X_ohe.csv')\n",
    "Xtest_ohe.to_csv('/data/fielddata/Xtest_ohe.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
